{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"17yyWlZ3jwbsjXwg0NEGWX-AgHWg9sA0D","authorship_tag":"ABX9TyOLdKIziFH4BMmABF5gGvS2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"id":"dJvGcdosUYqZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741942198964,"user_tz":-330,"elapsed":1005,"user":{"displayName":"Adil Mohammed Shajahan","userId":"05238904006533824158"}},"outputId":"212fb58b-b561-46b4-9796-71002eab1cab"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]}],"source":["#tokenisation\n","import nltk\n","from nltk.tokenize import word_tokenize\n","\n","nltk.download('punkt_tab')\n","\n","example_sent = \"\"\"This is a sample sentence,\n","                  showing off the stop words filtration.\"\"\"\n","word_tokens = word_tokenize(example_sent)\n","print(word_tokens)"]},{"cell_type":"code","source":["#stop word removal (a,an,the) which is not signifcant for training\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","print(stopwords.words('english'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nBOpGDCG20mx","executionInfo":{"status":"ok","timestamp":1741942516252,"user_tz":-330,"elapsed":218,"user":{"displayName":"Adil Mohammed Shajahan","userId":"05238904006533824158"}},"outputId":"4213abc4-2a0c-482a-843c-a861c0b2e716"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import string\n","\n","# Download necessary resources\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","example_sent = \"\"\"This is a sample sentence,\n","                  showing off the stop words filtration.\"\"\"\n","\n","# Load stopwords\n","stop_words = set(stopwords.words('english'))\n","\n","# Tokenize sentence\n","word_tokens = word_tokenize(example_sent)\n","\n","# Remove stopwords and punctuation (corrected approach)\n","filtered_sentence = [w for w in word_tokens if w.lower() not in stop_words and w not in string.punctuation]\n","\n","print(\"Original Tokens:\", word_tokens)\n","print(\"Filtered Tokens:\", filtered_sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n8IVp-kK3yfi","executionInfo":{"status":"ok","timestamp":1741942750922,"user_tz":-330,"elapsed":920,"user":{"displayName":"Adil Mohammed Shajahan","userId":"05238904006533824158"}},"outputId":"fc888f99-400e-4c20-f221-8ec1dec7b516"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Original Tokens: ['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n","Filtered Tokens: ['sample', 'sentence', 'showing', 'stop', 'words', 'filtration']\n"]}]},{"cell_type":"code","source":["#Stemming (removes suffixes like ing)\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt_tab')\n","from nltk.stem.snowball import SnowballStemmer\n","from nltk.tokenize import word_tokenize\n","\n","stemmer = SnowballStemmer(\"english\", True)\n","text = \"There is nothing either good or bad but thinking makes it so.\"\n","words = word_tokenize(text)\n","stemmed_words = [stemmer.stem(word) for word in words]\n","\n","print(\"Original:\", text)\n","print(\"Tokenized:\", words)\n","print(\"Stemmed:\", stemmed_words)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LJ6nq2nr4yE_","executionInfo":{"status":"ok","timestamp":1741942981294,"user_tz":-330,"elapsed":62,"user":{"displayName":"Adil Mohammed Shajahan","userId":"05238904006533824158"}},"outputId":"c4801317-e328-4aa5-a0cc-31b66ff3d3d3"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Original: There is nothing either good or bad but thinking makes it so.\n","Tokenized: ['There', 'is', 'nothing', 'either', 'good', 'or', 'bad', 'but', 'thinking', 'makes', 'it', 'so', '.']\n","Stemmed: ['there', 'is', 'noth', 'either', 'good', 'or', 'bad', 'but', 'think', 'make', 'it', 'so', '.']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}]},{"cell_type":"code","source":["#Lemmatization (exactly like stemming it distill words to foundation form)\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n","print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n","\n","# a denotes adjective in \"pos\"\n","print(\"better :\", lemmatizer.lemmatize(\"better\", pos=\"a\"))\n","# import these modules\n","from nltk.stem import WordNetLemmatizer\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n","print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n","\n","# a denotes adjective in \"pos\"\n","print(\"better :\", lemmatizer.lemmatize(\"better\", pos=\"a\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qi0qTF0G53Hh","executionInfo":{"status":"ok","timestamp":1741943320876,"user_tz":-330,"elapsed":45,"user":{"displayName":"Adil Mohammed Shajahan","userId":"05238904006533824158"}},"outputId":"f50798ab-e588-4d4e-f983-7ec775676832"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["rocks : rock\n","corpora : corpus\n","better : good\n","rocks : rock\n","corpora : corpus\n","better : good\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"markdown","source":[".\n","\n","**CHATBOT**"],"metadata":{"id":"_uX3hVPT73ML"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.chdir('/content/drive/MyDrive/Lab/Chatbot')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m-wmjzL97HV1","executionInfo":{"status":"ok","timestamp":1741944061523,"user_tz":-330,"elapsed":2154,"user":{"displayName":"Adil Mohammed Shajahan","userId":"05238904006533824158"}},"outputId":"d34949e3-b9f3-4632-cf99-111f4ee59305"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!pip install livelossplot --quiet"],"metadata":{"id":"7HfYHE5R7p-q","executionInfo":{"status":"ok","timestamp":1741944030673,"user_tz":-330,"elapsed":3920,"user":{"displayName":"Adil Mohammed Shajahan","userId":"05238904006533824158"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["#2 Importing Relevant Libraries\n","import json  # For loading and parsing JSON data\n","\n","import string  # For handling string operations, such as punctuation removal\n","import random  # For generating random numbers, useful in training data shuffling\n","\n","import nltk  # Natural Language Toolkit, used for text preprocessing\n","import numpy as np  # NumPy, for numerical computations and array manipulations\n","\n","from nltk.stem import WordNetLemmatizer  # For lemmatization (reducing words to their base form)\n","import tensorflow as tf  # TensorFlow, a deep learning framework\n","from tensorflow.keras import Sequential  # Sequential model for building neural networks\n","from tensorflow.keras.layers import Dense, Dropout  # Dense for fully connected layers, Dropout for regularization\n","\n","import livelossplot  # For real-time visualization of loss and accuracy during training\n","from livelossplot import PlotLossesKeras  # Keras callback to plot training progress\n","nltk.download(\"punkt\")\n","nltk.download('punkt_tab') ## Toke\n","nltk.download(\"wordnet\")   # dictionary for the English language; for lemmatization"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RSsokD7Y8NGX","executionInfo":{"status":"ok","timestamp":1741944019388,"user_tz":-330,"elapsed":13,"user":{"displayName":"Adil Mohammed Shajahan","userId":"05238904006533824158"}},"outputId":"1ab2e464-5cc3-4bf1-c5e9-25868354a297"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["#3 Loading the Dataset: intents.json\n","\n","data_file = open('intents.json').read()\n","data = json.loads(data_file)\n","\n","data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w-geOax8-ke-","executionInfo":{"status":"ok","timestamp":1741944413379,"user_tz":-330,"elapsed":9,"user":{"displayName":"Adil Mohammed Shajahan","userId":"05238904006533824158"}},"outputId":"c415c074-0210-4af2-bc19-4eaec78501b7"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'intents': [{'tag': 'hello',\n","   'patterns': ['Hello', 'Hi there', 'Good morning', \"What's up\"],\n","   'responses': ['Hey!', 'Hello', 'Hi!', 'Good morning!'],\n","   'context': ''},\n","  {'tag': 'noanswer',\n","   'patterns': [],\n","   'responses': [\"Sorry, can't understand you\",\n","    'Please give me more info',\n","    'Not sure I understand'],\n","   'context': ['']},\n","  {'tag': 'job',\n","   'patterns': ['What is your job', 'What is your work'],\n","   'responses': ['My job is to make you feel like everything is okay.',\n","    'I work to serve you as well as possible'],\n","   'context': ''},\n","  {'tag': 'age',\n","   'patterns': ['What is your age', 'How old are you', 'When were you born'],\n","   'responses': ['I was born in 2021'],\n","   'context': ''},\n","  {'tag': 'feeling',\n","   'patterns': ['How are you today', 'How are you'],\n","   'responses': ['I am feeling good, you?',\n","    'Very good and you?',\n","    \"Actually, I'm okay and you?\"],\n","   'context': ''},\n","  {'tag': 'good',\n","   'patterns': ['I am good too',\n","    'I feel fine',\n","    'Good !',\n","    'Fine',\n","    'I am good',\n","    'I am great',\n","    'great'],\n","   'responses': ['That is perfect!', \"So, everything's okay!\"],\n","   'context': 'feeling'},\n","  {'tag': 'bad',\n","   'patterns': ['I am feeling bad', 'No I am sad', 'No'],\n","   'responses': ['I hope you will feel better !'],\n","   'context': 'feeling'},\n","  {'tag': 'actions',\n","   'patterns': ['What can you do', 'What can I ask you', 'Can you help me'],\n","   'responses': ['I can do a lot of things but here are some of my skills, you can ask me: the capital of a country, its currency and its area. A random number. To calculate a math operation.'],\n","   'context': ''},\n","  {'tag': 'women',\n","   'patterns': ['Are you a girl', 'You are a women'],\n","   'responses': ['Sure, I am a women'],\n","   'context': ''},\n","  {'tag': 'men',\n","   'patterns': ['Are you a men', 'Are you a boy'],\n","   'responses': ['No, I am a women'],\n","   'context': ''},\n","  {'tag': 'thanks',\n","   'patterns': ['Thank you', 'Thank you very much', 'thanks'],\n","   'responses': ['I only do my job️', 'No problem!'],\n","   'context': ''},\n","  {'tag': 'goodbye',\n","   'patterns': ['Goodbye', 'Good afternoon', 'Bye'],\n","   'responses': ['Goodbye!', 'See you soon!'],\n","   'context': ''},\n","  {'tag': 'city',\n","   'patterns': ['Where do you live'],\n","   'responses': ['I live in a server located in the US!'],\n","   'context': ''},\n","  {'tag': 'action',\n","   'patterns': ['What are you doing'],\n","   'responses': [\"Actually, I'm chatting with somebody\"],\n","   'context': ''},\n","  {'tag': 'wait',\n","   'patterns': ['Can you wait 2 minutes', 'Please wait', 'Wait 2 secs please'],\n","   'responses': ['Sure! I wait.'],\n","   'context': ''},\n","  {'tag': 'still there',\n","   'patterns': ['Are you still there?', 'Are you here?'],\n","   'responses': ['Of course! Always at your service.'],\n","   'context': ''}]}"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.feature_extraction.text import CountVectorizer"],"metadata":{"id":"GWlnFoZI-nX7","executionInfo":{"status":"ok","timestamp":1741945034331,"user_tz":-330,"elapsed":49,"user":{"displayName":"Adil Mohammed Shajahan","userId":"05238904006533824158"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["corpus = [\n","    \"I love cats and dogs\",\n","    \"I love dogs\",\n","    \"Cats and dogs are amazing\"\n","]"],"metadata":{"id":"C0sg13gdA97f","executionInfo":{"status":"ok","timestamp":1741945256118,"user_tz":-330,"elapsed":5,"user":{"displayName":"Adil Mohammed Shajahan","userId":"05238904006533824158"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["vectorizer = CountVectorizer()  # Initialize BoW model\n","X = vectorizer.fit_transform(corpus)  # Fit and transform text into BoW vectors\n","\n","# Convert the sparse matrix to an array for readability\n","bow_array = X.toarray()\n","# Print the vocabulary (word index mapping)\n","print(\"Vocabulary:\", (vectorizer.vocabulary_))\n","\n","# Print the BoW representation of each sentence\n","print(\"Bag of Words Matrix:\\n\", bow_array)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SKmDfWZXB0EV","executionInfo":{"status":"ok","timestamp":1741945312810,"user_tz":-330,"elapsed":47,"user":{"displayName":"Adil Mohammed Shajahan","userId":"05238904006533824158"}},"outputId":"84c6f3e6-f542-4e59-fb35-0d2a6475b142"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary: {'love': 5, 'cats': 3, 'and': 1, 'dogs': 4, 'are': 2, 'amazing': 0}\n","Bag of Words Matrix:\n"," [[0 1 0 1 1 1]\n"," [0 0 0 0 1 1]\n"," [1 1 1 1 1 0]]\n"]}]},{"cell_type":"code","source":["#4 Extracting data_X(features) and data_Y(Target)\n","\n","words = [] #For Bow model/ vocabulary for patterns\n","classes = [] #For Bow  model/ vocabulary for tags\n","data_X = [] #For storing each pattern\n","data_y = [] #For storing tag corresponding to each pattern in data_X\n","# Iterating over all the intents\n","\n","for intent in data[\"intents\"]:\n","    print(intent)\n","    for pattern in intent[\"patterns\"]:\n","        print(pattern)\n","        tokens = nltk.word_tokenize(pattern) # tokenize each pattern\n","        words.extend(tokens\n",") #and append tokens to words\n","        data_X.append(pattern) #appending pattern to data_X\n","        data_y.append(intent[\"tag\"]) ,# appending the associated tag to each pattern\n","\n","    # adding the tag to the classes if it's not there already\n","    if intent[\"tag\"] not in classes:\n","        classes.append(intent[\"tag\"])\n","\n","# initializing lemmatizer to get stem of words\n","lemmatizer = WordNetLemmatizer()\n","\n","# lemmatize all the words in the vocab and convert them to lowercase\n","# if the words don't appear in punctuation\n","words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in string.punctuation]\n","# sorting the vocab and classes in alphabetical order and taking the # set to ensure no duplicates occur\n","words = sorted(set(words)) #vocabulary\n","classes = sorted(set(classes))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i6hmb0beEPtL","executionInfo":{"status":"ok","timestamp":1741945914031,"user_tz":-330,"elapsed":50,"user":{"displayName":"Adil Mohammed Shajahan","userId":"05238904006533824158"}},"outputId":"2c26fdbe-b3ff-4524-cffb-e0b2dbdb317d"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["{'tag': 'hello', 'patterns': ['Hello', 'Hi there', 'Good morning', \"What's up\"], 'responses': ['Hey!', 'Hello', 'Hi!', 'Good morning!'], 'context': ''}\n","Hello\n","Hi there\n","Good morning\n","What's up\n","{'tag': 'noanswer', 'patterns': [], 'responses': [\"Sorry, can't understand you\", 'Please give me more info', 'Not sure I understand'], 'context': ['']}\n","{'tag': 'job', 'patterns': ['What is your job', 'What is your work'], 'responses': ['My job is to make you feel like everything is okay.', 'I work to serve you as well as possible'], 'context': ''}\n","What is your job\n","What is your work\n","{'tag': 'age', 'patterns': ['What is your age', 'How old are you', 'When were you born'], 'responses': ['I was born in 2021'], 'context': ''}\n","What is your age\n","How old are you\n","When were you born\n","{'tag': 'feeling', 'patterns': ['How are you today', 'How are you'], 'responses': ['I am feeling good, you?', 'Very good and you?', \"Actually, I'm okay and you?\"], 'context': ''}\n","How are you today\n","How are you\n","{'tag': 'good', 'patterns': ['I am good too', 'I feel fine', 'Good !', 'Fine', 'I am good', 'I am great', 'great'], 'responses': ['That is perfect!', \"So, everything's okay!\"], 'context': 'feeling'}\n","I am good too\n","I feel fine\n","Good !\n","Fine\n","I am good\n","I am great\n","great\n","{'tag': 'bad', 'patterns': ['I am feeling bad', 'No I am sad', 'No'], 'responses': ['I hope you will feel better !'], 'context': 'feeling'}\n","I am feeling bad\n","No I am sad\n","No\n","{'tag': 'actions', 'patterns': ['What can you do', 'What can I ask you', 'Can you help me'], 'responses': ['I can do a lot of things but here are some of my skills, you can ask me: the capital of a country, its currency and its area. A random number. To calculate a math operation.'], 'context': ''}\n","What can you do\n","What can I ask you\n","Can you help me\n","{'tag': 'women', 'patterns': ['Are you a girl', 'You are a women'], 'responses': ['Sure, I am a women'], 'context': ''}\n","Are you a girl\n","You are a women\n","{'tag': 'men', 'patterns': ['Are you a men', 'Are you a boy'], 'responses': ['No, I am a women'], 'context': ''}\n","Are you a men\n","Are you a boy\n","{'tag': 'thanks', 'patterns': ['Thank you', 'Thank you very much', 'thanks'], 'responses': ['I only do my job️', 'No problem!'], 'context': ''}\n","Thank you\n","Thank you very much\n","thanks\n","{'tag': 'goodbye', 'patterns': ['Goodbye', 'Good afternoon', 'Bye'], 'responses': ['Goodbye!', 'See you soon!'], 'context': ''}\n","Goodbye\n","Good afternoon\n","Bye\n","{'tag': 'city', 'patterns': ['Where do you live'], 'responses': ['I live in a server located in the US!'], 'context': ''}\n","Where do you live\n","{'tag': 'action', 'patterns': ['What are you doing'], 'responses': [\"Actually, I'm chatting with somebody\"], 'context': ''}\n","What are you doing\n","{'tag': 'wait', 'patterns': ['Can you wait 2 minutes', 'Please wait', 'Wait 2 secs please'], 'responses': ['Sure! I wait.'], 'context': ''}\n","Can you wait 2 minutes\n","Please wait\n","Wait 2 secs please\n","{'tag': 'still there', 'patterns': ['Are you still there?', 'Are you here?'], 'responses': ['Of course! Always at your service.'], 'context': ''}\n","Are you still there?\n","Are you here?\n"]}]},{"cell_type":"code","source":["# 5 Text to Numbers\n","training = []\n","out_empty = [0] * len(classes) #out_empty: A list of zeros of length equal to the number of classes.\n","                                #This will be used to create a one-hot encoded vector for classification.\n","# creating the bag of words model\n","for idx, doc in enumerate(data_X):\n","    print(doc)\n","    bow = []\n","    text = lemmatizer.lemmatize(doc.lower())\n","    for word in words:\n","\n","        bow.append(1) if word in text else bow.append(0) #This creates a binary vector representation of the text.\n","    # mark the index of class that the current pattern is associated\n","    # to\n","    output_row = list(out_empty)\n","    output_row[classes.index(data_y[idx])] = 1 #one-hot encoded label vector\n","    # add the one hot encoded BoW and associated classes to training\n","    training.append([bow, output_row])\n","# shuffle the data and convert it to an array\n","random.shuffle(training)\n","training = np.array(training, dtype=object)\n","# split the features and target labels\n","train_X = np.array(list(training[:, 0]))\n","train_Y = np.array(list(training[:, 1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jWTRdMXVGbcV","executionInfo":{"status":"ok","timestamp":1741946518783,"user_tz":-330,"elapsed":6,"user":{"displayName":"Adil Mohammed Shajahan","userId":"05238904006533824158"}},"outputId":"c16b6b7f-e52b-41a6-a729-67a9c59fe60a"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello\n","Hi there\n","Good morning\n","What's up\n","What is your job\n","What is your work\n","What is your age\n","How old are you\n","When were you born\n","How are you today\n","How are you\n","I am good too\n","I feel fine\n","Good !\n","Fine\n","I am good\n","I am great\n","great\n","I am feeling bad\n","No I am sad\n","No\n","What can you do\n","What can I ask you\n","Can you help me\n","Are you a girl\n","You are a women\n","Are you a men\n","Are you a boy\n","Thank you\n","Thank you very much\n","thanks\n","Goodbye\n","Good afternoon\n","Bye\n","Where do you live\n","What are you doing\n","Can you wait 2 minutes\n","Please wait\n","Wait 2 secs please\n","Are you still there?\n","Are you here?\n"]}]}]}